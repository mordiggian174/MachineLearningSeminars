
Задача классификации проще задачи восстановления плотности распределения.

Модель байесовского классификатора - генеративная (учимся аппроксимировать вероятность каждого х при заданном у, а не наоборот)

Наивный байесовский классификатор - предполагаем, что все значения признаков каждого из классов независимо распределены по экспоненциальному
распределению и в итоге получаем некоторую линейную модель классификации, где коэффициенты считаются явно через оценку матожидания, 
а свободный член можно определить, например, через ROC-AUC score. 

Но подход байеса плох тем, что он пытается сравнивать плотности и на перифириии, даже когда они там малы у всех классов. 
По итогу восстановление в этой области неточное, а мы все равно берем аргмакс. Из-за этого часто возникают ошибки.

С гипотезой о нормальности распределений классов можно получить линейные и квадратичный дискриминанты.
Они просто получаются подстановкой плотностей с оценками матриц ковариаций. Линейный дискриминант ещё обладает и геометрической интерпретацией 
(Фишер получил по факту его).

GMM - предположение, что в каждом классе распределение - смесь с независимыми признаками по компонентам. Он приводит к формуле, 
которая очень похожа на метрический классификатор со взвешенным расстоянием и принадлежность классу = взвешенная принадлежность 
компонентам его смеси либо же на трехслойную нейронную сеть.
