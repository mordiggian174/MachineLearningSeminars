Третья парадигма машинного обучения помимо ошибки и вероятности - это принцип расстояний.
Для классификации, например, предполагается, что близкие объекты относятся к одному классу. Для регрессии, что близкие объекты имеют близкую метку. Основная задача - подобрать метрику и решатель.

Пример  - метод ближайшего соседа. Там смотрят на к ближайших элементов и берут класс доминирующий среди них. Параметр к можно подбирать, например, методом leave-one-out через минимизацию классификации всех известных объектов суммарно. 
Минусы - очень простой метод и не всегда хорошо работает, плюс логично бы не учитывать всех соседей равномерно, а давать по близости какие-то бонусы. Можно просто по порядку давать веса (линейно убывают или экспоненциально), а ещё логичнее давать веса в зависимости от расстояния!

Для этого начинают использовать функцию ядра, в которую засовывают расстояние деленное на длину так называемого окна.
Можно делать длину окно переменной величиной (расстояние от х до к+1 элемента, а учитывать только первые к). Также простор для выбора ядра.

Другой подход: каждый объект тренировочных данных делать источником запаха своего класса, располагая уникальное ядро из него. И классификация - это запаха какого класса больше всего. Параметры - ширина каждого ядра, вес ядра.
В случае двух классов эти ядра можно мыслить как моделирование новых признаков для объекта и линейную классификацию. А тогда можно через л1 регуляризацию выбрать лишь важные признаки = объекты, запах от которых мы хотим учитывать!

В принципе LOO должно быть достаточно для контроля kNN.
Они увидели на примере, что loo и cv в принципе ничем не отличаются (скорее всего, это особенность knn). 
Идея - делать потом отбор важных объектов, на которых лучше всего строить классификатор, путем минимизации ошибки для loo (это делать несложно)

Решение задачи регрессии с помощью метрических методов.
Прийти к непараметрическим моделям несложно: далеко не всегда мы знаем, как выглядит истинная зависимость и нам бывает тяжело предугадать архитектуру решения.
Тогда приходит идея считать решение константным, а в ошибке нормировать лоссы в зависимости от расстояния. Появляется аналитическое решение (Надарая-Ватсона, ядерное сглаживание). Ответ - средневзешенное меток обучающей выборки, веса - расстояние до объекта х. При некоторых ограничениях такой подход даже доказанно сходится к условному матожиданию.  Важно, что параметр ширины окна в ядре - очень важен, а вот ядра в целом при оптимальных широтах совпадают.

Борьба с выбросами:
Мы можем методом LOO обучать модели и смотреть, как сильно отличается предсказание этой модели с истинным значением. Отражнжировать по ошибкам, а затем либо исключить объекты из обучения, либо взвесить их коэффициентами равными какому-то другому ядру от ошибки! 
Метод LOWESS = Locally Weighted Scatter plot Smoothing (модифицируем веса в зависимости от степени выброса и пытаемся сгладить алгоритм). 
Например, квартическое ядро   
