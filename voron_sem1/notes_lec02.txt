Занимательно, что любая булева функция может быть реализована нейронной с двумя слоями.
Нейронная сесть с одной лишь нелинейной функцией активации способна аппроксимировать с любой точностью любую непрерывную функцию.
Двухслойные с функцией активацией сигмоидой также способны это делать (теорема Цыбенко).

Пример про метод обратного распространения ошибок. Он позволяет быстрее считать градиент. Но я не понял, в чем идея - то . Это же тривиальное ДП? 
Его плюсы: параллелится, быстро считается, обобщается.
Минусы: гиперпараметры сети, градиентного спуска, может долго сходиться, застревать в локальных минимумах, парализоваться.

Модификации метода с усреднением градиента типа моментума и нестеровской оптимизации.
Также для инициализации можно использовать трюк: обучать сначала первые слои независимо (подвыборки объектов или признаков), а затем второй слой.
Это можно воспринимать как Бэггинг + агрегация.

Есть методы с усреднением градиента - momentum, nag
Методы с нормировкой градиента по частоте обновлений (adaguard - идея), реализации - rms prop и
Ada delta - двойная нормировка

Также есть комбинации - momentum + rms prop = Adam (adaptive momentum)
Nadam = nag + rms prop

Можно повышать точность через метод обратного распространения ошибок, предположении о диагональности гессиана и метода Ньютона-Рафсона.
(Все это на -2- и -3-)


DropOut
Ещё интересный подход, навеянный биологией - иногда нейроны можно отключать для большей стабильности. Так мы заставим различные куски сети решать
поставленную задачу самостоятельно. Повышается надежность сети.

Обучение - на каждом шаге с какими-то вероятностями отключаем нейроны на слоях, обучаем сеть. Итоговая уже без отключения - там все нейроны взвешиваются 
вероятностью быть в сети. На практике часто делают обратную нормировку заранее, чтоб на итоговой сети ничего не делать (это так называемый
inverted dropout)

Аналог (optimal brain damage) 
Хотим также обнулить какой-то вес. При втором приближении функционала лосса и диагональности можно попытаться выкинуть самые незначительные веса
линейно, при этом структура сети упростится. При этом вести учет того, как сильно меняется ошибка. Если изменение слишком резкое (после адаптации через
повторный backpropogation - можно вернуть связи)

Важно: этот метод часто приводит к хорошему решению. Мы запускаем обучение, иногда прореживаем, обучаем и так далее.
Можно сделать аналог для нейронов, делать отбор признаков и так далее. Более аккуратный и долгий метод - это использовать весь гессиан (optimal
Brain surgeon).

Функции активации достаточно просто нелинейной. Тогда чтоб не было затухания градиента не используют с близкими асимптотами. Берут, например,
Relu или другие кусано линейные аналоги. Они показали себя хорошо

BatchNormalization
Мы знаем, что иногда нормализация данных полезна. Но если данных очень много, то на ум приходит банальная идея - нормировать по некоторым
пакетам (батчам). После нормировки запоминаются коэффициенты и они вставляются с настраиваемыми весами дополнительным слоем.
(Слой-то легкий, но вот размер сети возрастает в 2 раза + это все надо хранить).
