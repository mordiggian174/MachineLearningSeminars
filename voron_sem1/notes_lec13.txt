 Задача кластеризации - это обучение без учителя. Формального определения кластера нет, все очень сильно зависит от постановки, от функций расстояния и так далее. 
В каком-то смысле можно воспринимать её как задачу восстановления смеси распределения.	
Может применяться для разделения выборки для обучения на независимые части, либо для снижения размерности данных, определения шумовых объектов. 

Есть частичное обучение, в нем выделяют две постановки: либо мы учимся по части размеченных данных сразу строить классификатор, либо трансдуктивно обучать 
(то есть предсказывать метки только для неразмеченных данных, а не для произвольных).  
Важный пример, показывающий важность использования и неразмеченных данных: если в частичном обучении позволять пользоваться только размеченными данными, то, 
например, байесовский классификатор может оказаться смещенным.

Также показано, что в методах классификации кластерная структура слабо учитывается (два банана), и у разметки приоритет над ней (два банана с тремя метками).

Базовый алгоритм кластеризации: к-mean (просто выбрать к центров и минимизировать суммарные внутрикластерные расстояния с изменениями центров). 
Можно обобщить к частичному обучению, не присваивая размеченные объекты к ближайшему кластеру, а оставляя у себя. 
По факту k-mean - упрощение ЕМ алгоритма. Отличия в мягкой кластеризации и в отсутствии оптимизации дисперсий.
Важно понимать, что k-mean - это попытка описания кластеров сферическими Гауссианами.

DBSCAN - алгоритм кластеризации любой формы (density-based opacity clattering of application with noise).

Можно частичное обучение воспринимать как задачу кластеризации с ограничениями (нельзя иметь в кластере два размеченных по-разному объекта).

Есть алгоритмы иерархической кластеризации. Там просто поочередно сливают пары ближайших классов, расстояние мерится общей формулой Ланса-Уильямса 
(произвол в 3 константы). Визуализация на дендрограмме. С помощью нее уже можно при требовании детализации провести кластеризацию.

Для задач частичного обучения можно применять не только кластеризацию, но и классификацию (self training): мы можем поочередно делать прогнозы 
принадлежности классов и каждый раз расширять классы самыми объектами с большими  margin. Как вариант, делать два разных предсказателя, 
и один будет добавлять к себе в классы, в которых очень уверен другой.
Либо вообще композицию со взвешенным голосованием. Главное, чтобы методы  умели достаточно хорошо предсказывать на малых выборках (для начальных 
итераций это очень важно). 

Вообще общий подход для задач semi supervised Learning - конструировать функцию потерь отдельно для размеченных данных и отдельно для оставшихся. 
Таким образом учтутся все. 
Например, в бинарной классификации для неразмеченных данных вместо срезки 1-М будет браться 1-|M|, чтобы не учитывать знак класса (+-1) (transductive 
(от частного к частному) SVM).

Метод expectation regularization (XR). В нем идея следующая: определяемся с моделью классификации, имеем набор бинарных признаков (+- большой), и 
строим две разные оценки p(y | f_j = 1).
Одна по размеченным данным, вторая по предсказаниям модели на неразмеченных данных. Далее конструирует максимизация ожидания правдоподобия 
(там идет произведение по всем классам вероятностей pj(y|w)^индикатор, что модель именно в этом классе, а индикатор уже заменяется оценкой по
размеченным данным. Как итог - функционал зависит от параметров модели, обучается и на размеченных, и на неразмеченных данных.
 
