Приведен анализ смущённости и разброса ансамблей в случае квадратичного риска.
Идея градиентного бустинга - строим для произвольной функции потерь последовательно алгоритмы, уточняя предыдущие значения, через градиентный спуск.
Мы теперь каждый следующий алгоритм будем нацеливать на приближение вектора антиградиента (оставаясь решением в доступных нам моделях). 
Шаг выберем самостоятельно отдельной задачей оптимизацией. 
SGB - выбираем бэггингом подвыборки, по ним строим функционал ошибки.
Адабуст - частный случай GB

XGBoost - просто бустинг на деревьях с приближением лосса до 2го порядка + регуляризаторы.  Можно мыслить как градиентный бустинг 2го порядка.
Все сводится к оптимизации коньюнкций-условий попадания в лист, как это реализовано - непонятно.

Catboost - бустинг с борьбой от смещенности при подсчете градиента. Идея - достраивать ансамбль, построенный на выборке без этого объекта. 

  
Идея превратить категориальный признак в вещественный - записать  среднее значение. Реализация +- как в катбусте.
Используют как правило небрежные решающие деревья. В них удобно записывать условия принадлежности листу. 

Blending - взяли выборку. Отложили часть для агрегации. Построили по ней несколько базовых алгоритмов. Даем их как новые признаки какому-то алгоритму агрегации. 
На части данных не обучались (. Если обучаться сразу везде - то будет переобучение.

Stacking - делим выборку на k фолдов, каждый из которых даем базовым алгоритмам. На отложенном фолде получаем метапризнак и используем именно его для общей 
агрегации. Можно усреднять метапризнаки каждого из базовых алгоритмах на фолдах. И потом обучать на усредненных. Метапризнаки на тесте берутся из отдельного
обучения базовых алгоритмов на тесте.


Есть вариант при стэкинге брать линейную взвешенную комбинацию, где веса зависят от величин новых метапризнаков feauture-weighted linear stacking), а 
можно взвешивать базовые решения другими функциями-доверия (квазилинейный ансамбль)

Про ЕМ алгоритмы и смеси я скипаю. Основная идея - квазилинейность моделей. Как и FWLS.
