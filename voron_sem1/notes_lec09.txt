Про свойства независимых алгоритмов, их матожидание и дисперсии. Надо делать алгоритмы независимыми.

Bagging - мы формируем несколько выборок случайным образом из исходной с повторением. Это обеспечивает независимость полученных решений и их уже можно агрегировать.



Метод OOB = out of bag
После введения базового ансамбля (независимое обучение на подпространствах + среднее взвешивание) мы можем ввести понятие 
несмещённой оценки признака на объекте, если усредним значения базовых алгоритмов, которые этот объект не использовали для обучения.
После этого можно рассмотреть несмещенную оценку ошибки всего ансамбля, если посмотреть лоссы на этих ответах.

После этого можно оценивать важность j признака, путем сравнения OOB с перемешиванием этого признака и без.

Случайный лес (скорее стохастическое ансамблирование). Обучается на случайных подборках (Бэггинг) и признак выбирается из 
случайного подмножества признаков. Куча параметров оптимизации в случае деревьев - объемы выборок, количество деревьев,
размеры подпространств, глубины, критерии расщеплений, preprunning. Метод легко обобщать и параллелить, но если алгоритмы базовые устойчивы,
то могут получаться одинаковые модели :(. 

Бустинг для бинарной классификации. Строим алгоритмы последовательно, исправляя предыдущие ответы, а также функцию потерь аппроксимируем чем-то гладким. 
Если взять экспоненту, то получим AdaBoost. 

 Если вес какого-то объекта постоянно растет, то его можно распознать как выброс и вообще выкинуть из обучения.
У него есть хорошее свойство, что он непросто переобучается (так как он увеличивает отступы на каждом шаге). Это стало очень важным этапом в 
развитии машинного обучения.

Недостатки - очень чувствителен к выбросам из-за экспоненты. Неинтерпретируется. Упрощенный вариант - ComBoost: мы сразу ставим целью увеличивать отступы, 
при этом отбрасывая во время обучения очень хорошие и плохие.  Мы не использовали никакую аппроксимацию, не смотрим на шумы. Не появляются веса
объектов и алгоритмов. На экспериментах он выигрывает у ада буст.
