Откажемся от линейности и попробуем решить задачу регрессии методом второго порядка  Ньютона-Рафсона.
Пролинериазовав функцию, гессиан примет интересный вид: итерация на методе Ньютона-Рафсона - это решение МНК многомерной линейной регрессии 
(приближение через градиент вектора ошибки) ! (Назвали метод Ньютона-Гаусса)

Посмотрим теперь на логистическую регрессию. В ней метод Ньютона-Рафсона работает так, будто мы решаем задачу линейной многомерной регрессии МНК, 
где веса и значения взвешиваются. Более того, у этих весов есть вероятностная интерпретация: мы штрафуем и за неуверенность, и за ошибку!
Хоть функция и нелинейная, все равно метод Ньютона Рафсона сводит шаг к решению МНК линейной регрессионной задачи.
(Называется метод IRLS iteratively reweighted least squares)

Обобщенная аддитивная модель (GAM) - мы строим по исходным признакам новые (1 к 1), а затем линейную модель. Идея обучения новых признаков
поэтапно, с регуляризатором, стремящимся делать их линейными. Реализация - backfitting, инициализация из многомерной линейной регрессии, 
а дальше итерационный процесс уточнения каждого нового признака с некоторой регуляризацией (она очень важна!)

GLM - generalized linear model - обобщенная линейная модель
Вспоминаем, что МНК был обоснован лишь при гауссовском шуме. Попытаемся обобщить эту модель, введя экспоненциальное семейство распределений. 
Многие известные распределения являются экспоненциальными.  
Решая методом максимального правдоподобия и Ньютона-Рафсона приходим в очередной раз к задаче многомерной линейной регрессии (которую умеем решать,
например, через svd)

Это обосновывает и лог лосс, и сигмоидную функцию активации в задаче бинарной классификации , так как мы моментально приходим к предположению о 
распределении Бернулли, затем к экспоненциальному, а затем через метод максимального правдоподобия получаем требуемое.

Узнали про новые функции потерь. Например, функция Мешалкина является робастной, квантильная регрессия (скошенный модуль) 
