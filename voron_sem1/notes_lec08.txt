Новый подход к задачам машинного обучения:  логические методы.
Рассмотрим задачу классификации
Требования к логическому правилу:
Интерпретируемость (малое количество признаков участвует) и выделение некоторого класса (в некотором смысле).
Как такие решатель создаются? (Методика Rule Induction) Сначала определяются с допустимыми логическими правилами, 
затем определяются с алгоритмом выбора каких-то правил из этого семейства, 
договариваются о критерии информативность и потом агрегируют правила в одну модель.

Примеры - решающие пни, логические выражения ,синдромы (выполнение хотя бы д условий из семейства)

Критерием информативность может быть расположение в ПН пространстве (ниже и правее точек нет). Второй путь - попытаться агрегировать Н и П, 
например в метрики точности и полноты (зачем?).
При преодолении некоторого порога считать правило логической закономерностью. Также вводят статистический критерий (точный тест Фишера) - смотрят 
при гипотезе независимости вероятность того, 
что такие значения p,n получились. И аналогично при преодолении некоторого порога правило называют статистическим критерием. 
Статистический критерий более слабый, поэтому его используют при поиске правил,а логический при отборе (наверное это зависит ещё и от констант).
Ещё куча всяких критериев (Джини, IGain, с корнями)

Далее можно мыслить результаты правила как новый признак.

Приводится алгоритм построения дерева решений. Самое интересное там - это правило, по которому решается, стоит ли делить выборку или это лист. 
Там вводится аксиоматических мера неопределенности для текущих вершин, показывают пару функционалов, которые этим аксиомам удовлетворяют.

Важный плюс деревьев решений - они могут быть использованы и при наличии пропусков в данных: мы просто на этапе обучения пропускаем неподходящие объекты, 
а во время классификации пользуемся тривиальными оценками вероятности принадлежать тому или иному классу.

Минусы: не всегда оптимально строится + ближе к листьям объем выборки для обучения очередного правила уменьшается. Применяется post-pruning 
(усечение дерева после построения).

Посмотрим, как деревья могут решать задачу регрессии (аналогичный алгоритм, результат - кусочно-постоянная функция), мера неопределенности - МНК. 
Упрощение дерева делается через l1 регуляризатор (взвешенное количество листьев). Можно постепенно увеличивать этот коэффициент и выбрать оптимальное
дерево (через hold out выборку).

Пропускаем решающие списки и небрежные решающие деревья
