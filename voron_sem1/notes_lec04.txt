Метод опорных векторов. 
Очень важно, что вектор весов без свободного сдвига (за него мы не штрафуем!) Аппроксимацией функции ошибки берем срезку 
(1 - х).  К нему можно прийти и более математично через принцип максимизации ширины  полосы разделяющей гиперплоскости  
(задача квадратического программирования). А там через теорему Каруша-Куна-Таккера можно получить условия на экстремальную точку.
Далее переходим к двойственной задаче, получаем единственность решения. Видим, что везде фигурируют только скалярные произведения с опорными 
векторами. И это все приводит к важной идее: если мы докажем, что объекты даны какими-то попарными отношениями которые порождены скалярным 
произведением (некоторым), то мы можем воспользоваться этим методом.

Применение в нелинейности:
Мы ставим задачу в новом пространстве с новым скалярным произведением через ядра, там линейно разделяем через машину опорных векторов и получаем
(иногда нелинейную) классификацию в исходном пространстве. 

Если ядро представлено как функция от базового скалярного произведения - это можно мыслить как двухслойную нейронную сеть, где внутренний слой 
имеет нелинейную функцию - ядро от х и опорного вектора. Взяв различные ядра, можно получить как метод потенциальных функций (запахи объектов),
так и нейронную сеть с сигмоидой на скрытом слое

Минусы: подбор константы С, медленное обучение

Можно применять SVM и к регрессии с соответствующей функцией потерь - срезкой .
Также есть вариант SVM с L1 регуляризацией. Он отбирает признаки, но иногда может отбросить что-то важное взамен шуму. 
Есть вариант Elastic Net SVM - когда комбинируют сразу две нормы (можно не среднее, а взвесить как-то)
*Про эффект группировки я ничего не буду говорить*
Удачная комбинация - SFM (support features machine) , когда регуляризатор - это функционал, при малых значениях аргумента он ведет себя как модуль,
а при больших как парабола (получается такой гладкий клин).

Метод борьбы с отбором опорных векторов RVM (relevance vector machine) - вводится регуляризация с вероятностной модели как будто лямбды - случайные 
величины с гауссовским распределением и оптимизация по параметру дисперсии.
*Но вообще это странно так как матожидание не обязательно 0*

Самое главное замечание, что все эти двойственные задачи и теорема ККТ возникала из-за негладкости либо регуляризатора, либо функции потерь. Отсюда вывод: негладкость регуляризатора ведет к отбору признаков, а негладкость функции потерь к отбору объектов (svm).
