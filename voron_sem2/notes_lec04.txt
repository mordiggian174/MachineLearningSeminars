Ранее введенные рекуррентные сети не подходят для машинного перевода. Поэтому решили делать в два этапа (как автокодировщик), сначала собираем информацию
в один вектор со всего предложения, а потом, пользуясь ей, начинаем генерировать выходные данные (не обязательно той же размерности). Аналогия с
машинным переводом очевидна.

Модификация - позволять на этапе создания скрытых состояний выходной последовательности смотреть не только в последний элемент скрытого состояния входной
последовательности (прочитали все предложение), но и на все промежуточные, собрать из них вектор-контекст. Но это все дело по-прежнему не параллелится.

Модели внимания. 
С помощью механизма внимания можно научиться по запросу и всем ключам-словам строить требуемый контекст. Для этого применяются некоторые линейные 
преобразования к эмбеддингам слов, мерится их похожесть и от нее взвешивается «значение» слов -> вектору-контексту. 
Можно брать несколько независимых механизмов внимания и пытаться их агрегировать. Можно устраивать иерархию (но это я совсем не понял). 

Очень удачно механизм внимания применяется в трансформерах.
Нам даются данные, мы их векторизуем. Далее трансформер-кодировщик делает из них контексты, потом их контекстные представления декодирует трансформер в
другой язык и генерирует предложения в другом языке. 

В этом случае уже можно параллелить процесс, так как нет последовательного обучения. Контекстный эмбеддинг проходит для каждого из слов по отдельности
с помощью механизма внимания.

Декодировщик сначала обращает внимание на то, что он уже построил. Затем пытается по этому запросу обратить внимание на предсказания контекстов в другом языке,
настроив параметры их согласованности. Затем преобразует полученный «средний» контекст в образ вектора-представления слова. И затем от этой штуки путем софт
макса строятся вероятности иметь какое-то слово в данном месте. По итогу сэмплируется +- максимально вероятное слово на новом языке.
