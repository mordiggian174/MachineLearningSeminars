Хотим построить вероятностную модель для слова в заданном документе. Предполагаем, что есть фиксированное количество тем, и слово зависит лишь от нее.
Тогда можно перейти к матричному представлению (правда с стохастическими ограничениями).
Удивительно, что темы можно мыслить как мягкие кластеры в пространстве слов. И задача является представлением документов в пространстве тем. То есть
автокодировщиком +-.

Но задача матричного представления не имеет единственного решения, из-за чего можно накладывать регуляризатор для сужения области решений. Тогда можно
записать опять же итерационный ЕМ алгоритм, который очень хорошо интерпретируется: там даже есть переменная, отвечающая за то, «что имел в виду писатель,
когда писал данное слово в этом документе», как часто слово попадалось в данной теме, как часто тема встречалась в документе.

Базовые регуляризаторы - никакой (plsa), и через распределение Дирихле (lda). Но у них есть более интересные аналоги:

Из лингвистических соображений строится регуляризатор, старающийся построить общие и специализированные темы (разряжая и сглаживая параметры матриц). 
К этим формулам пришли от распределения Дирихле.


Можно ввести и регуляризатор, разряживающий темы. Можно ввести регуляризатор, который позволит документам давать категории или решать задачу регрессии
(интересное применение - по потоку новостей предсказывать индекс цен).

ARTM Рассматривается также задача поиска порождения таких тем, что смогут описать несколько разных модальностей. Так, например, можно делать 
мультиязычное тематизирование: берем две версии статьи на разных языках, объединяем их в один документ. Очевидно, что потери тем при этом не произошло. 
Только теперь слова разных языков воспринимаем как разные токена разных модальностей.  

Можно потом учитывать ссылки одних документов на другие в регуляризаторе, либо строить иерархию тем
