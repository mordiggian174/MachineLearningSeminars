Важно, что теперь появилось время во всех выборках. Надо будет это отметить.

Задача по истории научиться предсказывать новый временной интервал. Хочется быстро перестраивать модели. Как пример - смотреть только на n
предыдущих событий (н ещё и подбирать надо, чтоб не словить мультиколлинеарность). Можно построить линейную регрессионную модель.

В случае, когда мы надеемся на стационарную модель, можно бахнуть константное решение с экспоненциально убывающим весом прошлых событий - получится
экспоненциальное среднее 


Альфа отвечает за ширину окна прошлого. Его можно оптимизировать по известной истории.

Но не всегда модель просто стационарна. Можно увидеть, например, линейный тренд (Хольт)

Можно учитывать аддитивную сезонность (Тейла-Вейджа), введя новые параметры.  А можно и мультипликативную (Уинтерс). Совмещать их. Есть даже экспоненциальные
тренды. Для всех из них выписываются рекуррентные формулы по параметрам (все очень похоже на экспоненциально скользящее среднее). Параметры оптимизируются
по данным.

Совсем другой подход: мы обучаем линейную модель, но оптимизирует при изменении выборки только вдоль нового объекта (градиентным шагом). Что-то типа бустинга
линейной модели на новом объекте. 

Предложен следящий контрольный сигнал - метод проверки, насколько хорошо модель предсказывает среднее значение ряда. Это отношение экспоненциального среднего
ошибки и модуля ошибки. Если оно маленькое - значит все хорошо.

Модель Тригга-Лича. Этот показатель можно использовать, чтобы понять, что в какой-то момент произошло резкое изменение и модели надо резко поменять предсказания!
(Но он и на шум посмотрит)

Агрегация можно просто брать лучшую с точки зрения ЭСС модуля ошибки (селекция), а можно агрегировать ответы разных моделей в один новый линейной моделью.
Можно коэффициенты делать пропорциональными их ЭСС модулей ошибок, а можно обучать регрессией. Интересное условие неотрицательности хорошо интерпретируется
и показывает результаты.
