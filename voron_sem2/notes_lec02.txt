Самый базовый алгоритм кластеризации как предсказание одного из Т классов по ближайшему из центров с критерием ошибки - внутрикластерным расстоянием. 
Его можно мыслить и как двухслойную нейронную сеть (сеть Кохонена). Там шаг стохастического градиентного спуска будет выглядеть как смещение центра
предсказанного класса. Модификация - смещать все центры, но в пропорционально расстоянию от выбранного объекта (опять ядра). 
Для визуализации ещё придумали карты Кохонена. Там вводится сетка из центров кластеров, и в алгоритме смещаются только близкие по сетке кластеры. 
Потом проводится визуализация по координатам векторов-центров каждого из узлов сети, ищется зависимость значения координаты с кластером.

Новая задача (автокодировщик) - другими словами сжатие данных. Это обобщение метода главных компонент: нелинейность,  отсутствие требование на
транспонирование матриц и ортогональности. 
Бывают и кодировщики с повышением размерности, но там обычно требуют разреженность (sparse ae) и пытаются интерпретировать нули (например, через
регуляризацию л1 промежуточного представления).
(denoising ae) - мы хотим близость объекта с представлением его    зашумленной копии.

(Contractive ae) - требуем сжатия от отображения в сжатое представление (видимо для устойчивости к шуму опять же), достигается путем регуляризации 
нормой матрицы якоби.
(Relation ae) - требуем в представлении сохранения отношения между некоторыми объектами
Вариационный автокодировщик - он пытается построить сразу генеративную модель, чтоб генерировать естественные похожие объекты. И кодировщик и
декодировщик - вероятностные. Максимизируются правдоподобие, там вводится для простоты что представление - это детерминированная функция + шум (гауссовский).
И оптимизация sg.

Если хотим представление использовать для классификации то можем скомбинировать два лосса.


Ещё одна новая идея - трансферное обучение. Мы можем использовать универсальную часть какой-то большой модели в качестве кирпича в своем решении.
Например, в большой нейронной сети весь этап векторизации взять как кодировщик, а обучать потом только декодировщик.

Похожая идея - мультизадачное обучение.
У нас в нескольких задачах со своими выборками и функциями потерь может быть один общий этап (векторизация), и мы просто оптимизирует сразу лосс по
всем задачам, где решатель первого этапа - общий. Важный бонус - few shot Learning (нам для новой задачи может понадобиться всего пару экземпляров).

Self-supervised learning - идея, что иногда в исходных данных уже сложная структура и мы можем, расчленяя объекты, нагенерировать огромные данные и
обучить сильную модель для векторного представления исходных данных.

Дистилляция или суррогатное моделирование - мы сначала обучаем очень сильную и сложную модель, а затем аппроксимируем по другой выборке именно её предсказания,
а не исходную разметку. Плюсы - мы можем генерировать размеченные данных любого объема.

Обучение с привилегированной информацией - это когда для тренировочных данных у нас есть доп. инфа. Тогда мы можем пытаться обучать ученика предсказывать 
как учитель (с доступом к этой инфекции), либо же обучать их совместно.
  
Ну и ГАНы я знал. Там подход генеративный статистический. Он выигрывает во многом у вариационных автоэнкодеров по генерации естественных фейковых объектов.
