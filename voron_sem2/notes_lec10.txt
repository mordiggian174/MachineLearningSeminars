Есть множество действий, распределение наград и состояний, хотим построить оптимальную стратегию действий.

Тривиальный способ - вести подсчет стоимости каждого из состояний и брать на новом шаге действие с максимальной стоимостью, но она плоха
(нет исследований и изменений).

Можно модернизировать эпсилон-жадной. Также есть вариации с не просто частотной оценкой выгодности действия, а экспоненциальной или UCB.

В случае, когда состояние среды меняется, можно вводить функции ценности действий и действий в состоянии. Для них появляется формула рекуррентная,
которая сводится к уравнению Беллмана. Утверждается, что жадная по Беллману стратегия оптимальна. Остается лишь оценивать функционал Беллмана:
можно делать это по текущей стратегии (sarsa, on-policy) алгоритмы, можно брать честный максимум (q Learning, off-policy).

Есть вариации с нейронными сетями, моделирующими функционал Беллмана. Там замечательный момент, что мы узнаем по текущей стратегии новое состояние,
затем оцениваем функционал и метимся на него, меняя параметры. То есть по факту, наша целевая метка зависит от параметров самой сети (текущего шага)! 

Можно пытаться искать политику в параметризованном виде и делать градиентные спуски (policy gradient). Там путем преобразований можно свести всё к
максимизации матожидания. Его оцениваем через экспоненциальное среднее.

Самый общий случай - научиться моделировать систему целиком. Например, линейной моделью: вводим вектор x_at (пара состояние и действие) и w_a(параметр модели).
Вариации с учетом внешних факторов и верхних доверительных оценок.

Проблемы с онлайн и офлайн оцениваем: простое (но тупое) решение  - это оценивать политики только по совпавшим действиям.

