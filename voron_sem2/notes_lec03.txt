Интересная идея, что смысл слова - это всевозможные контексты, где оно появляется. Парадигматическая близость слов - это если мы прям можем заменить слова,
а синтагматическая - это если мы видим слова постоянно в близких контекстах (функция - точка).

В методе CBOW пытаются по контексту предсказать слово внутри через МЕ (типа в контексте пропущенное слово близко к среднему), а в skip-grame’e по
слову предсказать ему контекст.
Важно, что у каждого слова есть две версии представления: когда оно участвует в предсказании и когда его пытаются предсказать. Думаю, это связано с тем, 
что в первом случае в представлении содержится больше информации по обобщающей способности. Но реализация сложная из-за softmax’a: он по всему тексту вычисляется.

Там очень интересное решение Иерархический softmax - представить словарь как бинарное дерево. И во время пути к какому-то слову w, мы вместе с переходами 
в словаре производили бы некоторые вычисления таким образом, чтобы в конце сразу было p(w | w_i). В целом, это просто серьезная предобработка, позволяющая
считать p(w|w_i) за лог словаря (но не было доказательства, что это всегда можно сделать, я не считаю это тривиальным фактом).

И ещё один подход - это генерировать случайным образом примеры из не контекста и пытаться обучать классификатор «в контексте» и «не в контексте». 
Skip-gram Negative Sampling 

Удивительный факт, что такие представления слов являются приближением следующей матрицы взаимной информации 
По факту тут логарифм отношения вероятности встретить сразу пару в контексте к вероятности встретить их по от
дельности в одном контексте (проверка на независимость).

FastText: Ещё одна идея - делать векторное представление не каждого отдельного слова, а некоторой комбинации букв, то есть буквенным n-граммам. 
Преимущества - их для каждого конкретного языка уже намного меньше, чем слов, + борется с опечатками и новыми словами.

После этого заметили, что нам не так важна структура слова-текст, мы используем лишь частоты появления слова в контексте. Для пары слов нам важно знать,
часто они возникают или нет. Это привело к структуре графа и обобщению этих подходов на другие области: рекомендательные системы, социальные сети и др.

Постановка задачи: для объектов даны расстояния и мы хотим перенести их в евклидово пространство с сохранением расстояний. 
Похожая задача - когда мы мерим разницу исходных расстояний и скалярного произведения образов. Это уже задача матричного разложения.

Одно из решений (stochastic neighbor embedding)- перейти к статистической постановке, в которой вероятность появления связи ij зависит от расстояние 
(через Гауссианы). Важно, что там гауссианы в исходном пространстве у всех точек свои, в зависимости от плотности соседей (нормировка через дисперсии).
И мы хотим приближать так, чтобы эта вероятность в исходном пространстве и пространстве образов были похоже (через ЕМ). Минус гауссиан - хвосты слишком 
быстро убывают и мы сильно штрафуем лосс за далекие объекты, решение - взять другое распределение т-Стьюдента и немного другой лосс (t SNE).

Можно в задачах на графе пользоваться и skip-gram negative-sampling, где контекст уже будет не как в тексте, а через случайное блуждание. 
Реализовать можно по-разному, например, через параметризация обхода в глубину и в ширину. 


Самая общая схема очень похожа на работу автокодировщика: мы имеем на входе объекты и данные об их отношениях. Далее можем решать как задачу с 
учителем при промежуточном представлении данных в другой размерности, так и задачу снижения размерности для данных об отношениях объектов. 
