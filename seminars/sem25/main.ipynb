{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "main.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "5ZRb9MHLET76",
        "_OkILcn8Dur5",
        "vtzAdAM2QNE4"
      ],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/andriygav/MachineLearningSeminars/blob/master/sem25/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z70JPkogCtCa"
      },
      "source": [
        "# Обучение с подкреплением"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20Juwpv2DSGN"
      },
      "source": [
        "## Библиоттеки"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iJZS6NtgCvzw"
      },
      "source": [
        "from abc import abstractmethod\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SV7EjK3LCwNK"
      },
      "source": [
        "## Байесовские однорукие бандиты"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4EqdeDSMDZoY"
      },
      "source": [
        "Простейшая задача --- бандиты с распределением Бернулли.\n",
        "Бандиты это окружение с $K$ действиями. Действие приводит к вознаграждению $r=1$ с вероятностью $0 \\le \\theta_k \\le 1$, неизвестной агенту, но фиксированной во времени. Цель агента - минимизировать неоптимальность на фиксированном горизонте из $T$ действий:\n",
        "$$\\rho = T\\theta^* - \\sum_{t=1}^T r_t,  \\qquad \\theta^* = \\max_k\\{\\theta_k\\}.$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ZRb9MHLET76"
      },
      "source": [
        "### Определим исследователя среды"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YHMg8EifEW4u"
      },
      "source": [
        "class AbstractAgent:   \n",
        "    def __init__(self, n_actions, seed=42):\n",
        "        self.rs = np.random.RandomState(seed)\n",
        "\n",
        "        self._successes = np.ones(n_actions)\n",
        "        self._failures = np.ones(n_actions)\n",
        "        self._total_pulls = 0\n",
        "        \n",
        "    def update(self, action, reward):\n",
        "        self._total_pulls += 1\n",
        "        if reward == 1:\n",
        "            self._successes[action] += 1\n",
        "        else:\n",
        "            self._failures[action] += 1\n",
        "\n",
        "    @abstractmethod\n",
        "    def get_action(self):\n",
        "        pass\n",
        "\n",
        "    @property\n",
        "    def name(self):\n",
        "        return self.__class__.__name__\n",
        "\n",
        "    @property\n",
        "    def total(self):\n",
        "        return self._total_pulls\n",
        "\n",
        "    @property\n",
        "    def success(self):\n",
        "        return self._successes\n",
        "\n",
        "    @property\n",
        "    def failure(self):\n",
        "        return self._failures"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tuCglYP0CE27"
      },
      "source": [
        "#### Default агент"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H_8BYclACEbx"
      },
      "source": [
        "class DefaultAgent(AbstractAgent):\n",
        "    def get_action(self):\n",
        "        return np.argmax(\n",
        "                self.success/(self.success + self.failure))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yxf3gdDDFd8T"
      },
      "source": [
        "#### Epsilon-greedy агент"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "09887PYBFgVC"
      },
      "source": [
        "class EpsilonGreedyAgent(AbstractAgent):\n",
        "    def __init__(self, K, seed=42, epsilon=0.01):\n",
        "        super().__init__(K, seed)\n",
        "        self._epsilon = epsilon\n",
        "\n",
        "    def get_action(self):\n",
        "        if self.rs.rand() < self._epsilon:\n",
        "            return self.rs.randint(0, self.success.shape[0])\n",
        "        else:\n",
        "            return np.argmax(self.success/(self.success + self.failure))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p4cT3EjFGgnO"
      },
      "source": [
        "#### UCB агент"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_jnQxxLXGdvs"
      },
      "source": [
        "class UCBAgent(AbstractAgent):\n",
        "    def get_action(self):\n",
        "        if self.total != 0:\n",
        "            return np.argmax(\n",
        "                self.success/(self.success + self.failure) \\\n",
        "                + np.sqrt(2*np.log(self.total)/(self.success + self.failure)))\n",
        "        else:\n",
        "            return np.argmax(\n",
        "                self.success/(self.success + self.failure))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7sfKYGguHY7I"
      },
      "source": [
        "#### Thompson sampling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zW7fGPxhG-5U"
      },
      "source": [
        "class ThompsonSamplingAgent(AbstractAgent):\n",
        "    def get_action(self):\n",
        "        return np.argmax(self.rs.beta(self.success, self.failure))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_OkILcn8Dur5"
      },
      "source": [
        "### Стационарный бандит"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q1jcKxdsDO4p"
      },
      "source": [
        "class BernoulliBandit:\n",
        "    def __init__(self, K=5, seed=42):\n",
        "        self.rs = np.random.RandomState(seed)\n",
        "        self._probas = self.rs.random(K)\n",
        "\n",
        "    @property\n",
        "    def action_count(self):\n",
        "        return len(self._probas)\n",
        "\n",
        "    def pull(self, action):\n",
        "        return self.rs.choice([0, 1], p=[1-self._probas[action], self._probas[action]])\n",
        "\n",
        "    def optimal_reward(self):\n",
        "        return np.max(self._probas)\n",
        "\n",
        "    def step(self):\n",
        "        pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iX30-1tGHxH8"
      },
      "source": [
        "agents = [\n",
        "    DefaultAgent,\n",
        "    EpsilonGreedyAgent,\n",
        "    UCBAgent,\n",
        "    ThompsonSamplingAgent\n",
        "]\n",
        "n_steps  = 5000\n",
        "n_trials = 10\n",
        "K        = 5\n",
        "\n",
        "scores = {\n",
        "    agent(K).name : [0.0 for step in range(n_steps)] for agent in agents\n",
        "}\n",
        "\n",
        "for trial in range(n_trials):\n",
        "    for agent in agents:\n",
        "        bandit = BernoulliBandit(K, seed=trial)\n",
        "        a = agent(K, seed=trial)\n",
        "\n",
        "        for i in range(n_steps):\n",
        "            optimal_reward = bandit.optimal_reward()\n",
        "\n",
        "            action = a.get_action()\n",
        "            reward = bandit.pull(action)\n",
        "            a.update(action, reward)\n",
        "\n",
        "            scores[a.name][i] += optimal_reward - reward\n",
        "                \n",
        "            bandit.step()\n",
        "\n",
        "plt.figure(figsize=(17, 8))\n",
        "for agent in agents:\n",
        "    plt.plot(np.cumsum(scores[agent(K).name]) / n_trials, label=agent(K).name)\n",
        "\n",
        "plt.legend(loc='best')\n",
        "\n",
        "plt.ylabel(\"regret\")\n",
        "plt.xlabel(\"steps\")\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vtzAdAM2QNE4"
      },
      "source": [
        "### Нестационарный бандит"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wRHtXpqoPm5b"
      },
      "source": [
        "class DriftingBandit(BernoulliBandit):\n",
        "    def __init__(self, K=5, seed=42, gamma=0.01):\n",
        "        super().__init__(K, seed)\n",
        "        \n",
        "        self._gamma = gamma\n",
        "        \n",
        "        self._successes = np.ones(self.action_count)\n",
        "        self._failures = np.ones(self.action_count)\n",
        "        self._steps = 0\n",
        "    \n",
        "    def step(self):\n",
        "        action = self.rs.randint(self.action_count)\n",
        "        reward = self.pull(action)\n",
        "\n",
        "        self._successes = self._successes * (1 - self._gamma) + self._gamma\n",
        "        self._failures = self._failures * (1 - self._gamma) + self._gamma\n",
        "        self._steps += 1\n",
        "\n",
        "        self._successes[action] += reward\n",
        "        self._failures[action] += 1.0 - reward\n",
        "\n",
        "        self._probas = self.rs.beta(self._successes, self._failures)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kySK3BhjQ0zn"
      },
      "source": [
        "agents = [\n",
        "    DefaultAgent,\n",
        "    EpsilonGreedyAgent,\n",
        "    UCBAgent,\n",
        "    ThompsonSamplingAgent\n",
        "]\n",
        "n_steps  = 5000\n",
        "n_trials = 10\n",
        "K        = 5\n",
        "\n",
        "scores = {\n",
        "    agent(K).name : [0.0 for step in range(n_steps)] for agent in agents\n",
        "}\n",
        "\n",
        "for trial in range(n_trials):\n",
        "    for agent in agents:\n",
        "        bandit = DriftingBandit(K, seed=trial)\n",
        "        a = agent(K, seed=trial)\n",
        "\n",
        "        for i in range(n_steps):\n",
        "            optimal_reward = bandit.optimal_reward()\n",
        "\n",
        "            action = a.get_action()\n",
        "            reward = bandit.pull(action)\n",
        "            a.update(action, reward)\n",
        "            scores[a.name][i] += optimal_reward - reward\n",
        "                \n",
        "            bandit.step()\n",
        "\n",
        "plt.figure(figsize=(17, 8))\n",
        "for agent in agents:\n",
        "    plt.plot(np.cumsum(scores[agent(K).name]) / n_trials, label=agent(K).name)\n",
        "\n",
        "plt.legend(loc='best')\n",
        "\n",
        "plt.ylabel(\"regret\")\n",
        "plt.xlabel(\"steps\")\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yguSf4FpRHEY"
      },
      "source": [
        "#### Агент учитывающий нестационарность"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vI30H5nXQ5y7"
      },
      "source": [
        "class BrakeMemoryAgent(AbstractAgent):\n",
        "    def get_action(self):\n",
        "        if self.total % 700 == 0:\n",
        "            self._successes=self._successes*0+1\n",
        "            self._failures=self._failures*0+1\n",
        "        return np.argmax(self.rs.beta(self._successes, self._failures))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "leV0ZcgkRVUD"
      },
      "source": [
        "agents = [\n",
        "    DefaultAgent,\n",
        "    EpsilonGreedyAgent,\n",
        "    UCBAgent,\n",
        "    ThompsonSamplingAgent,\n",
        "    BrakeMemoryAgent\n",
        "]\n",
        "n_steps  = 5000\n",
        "n_trials = 10\n",
        "K        = 5\n",
        "\n",
        "scores = {\n",
        "    agent(K).name : [0.0 for step in range(n_steps)] for agent in agents\n",
        "}\n",
        "\n",
        "for trial in range(n_trials):\n",
        "    for agent in agents:\n",
        "        bandit = DriftingBandit(K, seed=trial)\n",
        "        a = agent(K, seed=trial)\n",
        "\n",
        "        for i in range(n_steps):\n",
        "            optimal_reward = bandit.optimal_reward()\n",
        "\n",
        "            action = a.get_action()\n",
        "            reward = bandit.pull(action)\n",
        "            a.update(action, reward)\n",
        "            scores[a.name][i] += optimal_reward - reward\n",
        "                \n",
        "            bandit.step()\n",
        "\n",
        "plt.figure(figsize=(17, 8))\n",
        "for agent in agents:\n",
        "    plt.plot(np.cumsum(scores[agent(K).name]) / n_trials, label=agent(K).name)\n",
        "\n",
        "plt.legend(loc='best')\n",
        "plt.ylabel(\"regret\")\n",
        "plt.xlabel(\"steps\")\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cb6vpn5aS3XS"
      },
      "source": [
        "## Задача о заплыве"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QUBvsySfTDT2"
      },
      "source": [
        "Задача на основе марковских процесов.\n",
        "\n",
        "https://arxiv.org/pdf/1306.0940.pdf"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hhn4b5VERZGN"
      },
      "source": [
        "class RiverSwim:\n",
        "    def __init__(self, \n",
        "                 isc  = 4, # число состояний (без учета крайних)\n",
        "                 ms   = 16, # максимальное число шагов\n",
        "                 lr   = 5/1000, \n",
        "                 rr   = 1.0, \n",
        "                 seed = 42):\n",
        "        self.rs = np.random.RandomState(seed)\n",
        "        self._max_steps = ms\n",
        "        self._interm_states = isc\n",
        "\n",
        "        self._steps = 0\n",
        "        self._current_state = 1\n",
        "\n",
        "        self.lr = lr\n",
        "        self.rr = rr\n",
        "        \n",
        "    @property\n",
        "    def action_count(self):\n",
        "        return 2\n",
        "    \n",
        "    @property\n",
        "    def states_count(self):\n",
        "        return 2 + self._interm_states\n",
        "    \n",
        "    def _get_probs(self, action):\n",
        "        if action == 0:\n",
        "            if self._current_state == 0:\n",
        "                return [0.00, 1.00, 0.00]\n",
        "            else:\n",
        "                return [1.00, 0.00, 0.00]\n",
        "            \n",
        "        elif action == 1:\n",
        "            if self._current_state == 0:\n",
        "                return [0.00, 0.40, 0.60]\n",
        "            if self._current_state == self.states_count - 1:\n",
        "                return [0.40, 0.60, 0.00]\n",
        "            else:\n",
        "                return [0.05, 0.60, 0.35]\n",
        "\n",
        "    def step(self, action):\n",
        "        reward = 0.0\n",
        "        if self._steps >= self._max_steps:\n",
        "            return self._current_state, reward, True\n",
        "        \n",
        "        transition = self.rs.choice(range(3), p=self._get_probs(action))\n",
        "        if transition == 0:\n",
        "            self._current_state -= 1\n",
        "        elif transition == 1:\n",
        "            pass\n",
        "        else:\n",
        "            self._current_state += 1\n",
        "\n",
        "        if self._current_state == 0:\n",
        "            reward = self.lr\n",
        "        elif self._current_state == self.states_count - 1:\n",
        "            reward = self.rr\n",
        "        \n",
        "        self._steps += 1\n",
        "        return self._current_state, reward, False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lp-h9hQ3WAyw"
      },
      "source": [
        "class QLearningAgent:\n",
        "    def __init__(self, n, K, lr=0.2, gamma=0.95, epsilon=0.2, seed=42):\n",
        "        self.rs = np.random.RandomState(seed)\n",
        "        self._gamma = gamma\n",
        "        self._epsilon = epsilon\n",
        "        self._q_matrix = np.zeros((n, K))\n",
        "        self._lr = lr\n",
        "\n",
        "    def get_action(self, state):\n",
        "        if self.rs.random() < self._epsilon:\n",
        "            return self.rs.randint(0, self._q_matrix.shape[1])\n",
        "        else:\n",
        "            return np.argmax(self._q_matrix[state])\n",
        "    \n",
        "    def get_q_matrix(self):\n",
        "        return self._q_matrix\n",
        "        \n",
        "    def start_episode(self):\n",
        "        pass\n",
        "\n",
        "    def update(self, state, action, reward, next_state):\n",
        "        self._q_matrix[state, action] = self._q_matrix[state, action] + self._lr*(reward + self._gamma*np.max(self._q_matrix[next_state]) - self._q_matrix[state, action])\n",
        "        return\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C4FuuDBXWeUr"
      },
      "source": [
        "n = 2\n",
        "ms = 128\n",
        "episodes = 500\n",
        "actions = 2\n",
        "\n",
        "agent = QLearningAgent(2+n, actions)\n",
        "\n",
        "episode_rewards = []\n",
        "for ep in range(episodes):\n",
        "    river_swim = RiverSwim(n, ms)\n",
        "\n",
        "    state, ep_reward, is_done = river_swim._current_state, 0.0, False\n",
        "    agent.start_episode()\n",
        "    while not is_done:\n",
        "        action = agent.get_action(state)\n",
        "        next_state, reward, is_done = river_swim.step(action)\n",
        "        agent.update(state, action, reward, next_state)\n",
        "        \n",
        "        state = next_state\n",
        "        ep_reward += reward\n",
        "        \n",
        "    episode_rewards.append(ep_reward)\n",
        "\n",
        "\n",
        "plt.figure(figsize=(15, 8))\n",
        "\n",
        "plt.plot(pd.DataFrame(np.array(episode_rewards)).ewm(alpha=.1).mean())\n",
        "plt.xlabel(\"Episode count\")\n",
        "plt.ylabel(\"Reward\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ye0DvZo7Xw_n"
      },
      "source": [
        "fig = plt.figure(figsize=(15, 8))\n",
        "ax = fig.add_subplot(111)\n",
        "ax.matshow(agent.get_q_matrix().T)\n",
        "ax.set_yticklabels(['', 'no act', 'swim'])\n",
        "plt.xlabel(\"State\")\n",
        "plt.ylabel(\"Action\")\n",
        "plt.title(\"Values of state-action pairs\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}